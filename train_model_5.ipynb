{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   Copyright 2020 Erik Lee\n",
    "\n",
    "   Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "   you may not use this file except in compliance with the License.\n",
    "   You may obtain a copy of the License at\n",
    "\n",
    "       http://www.apache.org/licenses/LICENSE-2.0\n",
    "\n",
    "   Unless required by applicable law or agreed to in writing, software\n",
    "   distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "   See the License for the specific language governing permissions and\n",
    "   limitations under the License."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import zipfile\n",
    "import random\n",
    "import shutil\n",
    "from shutil import copyfile\n",
    "import cv2.cv2 as cv2\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib as plt\n",
    "from tensorflow.keras import layers, models, regularizers\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from stn import spatial_transformer_network as transformer\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import pandas\n",
    "from tensorflow.keras.applications import InceptionV3\n",
    "\n",
    "i_s = 128\n",
    "tf.keras.backend.clear_session()\n",
    "%precision 4\n",
    "\n",
    "L2_WEIGHT_DECAY = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    except RuntimeError as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Model: \"sequential_1\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nconv2d_4 (Conv2D)            (None, 126, 126, 64)      1792      \n_________________________________________________________________\ndropout_2 (Dropout)          (None, 126, 126, 64)      0         \n_________________________________________________________________\nbatch_normalization_6 (Batch (None, 126, 126, 64)      256       \n_________________________________________________________________\nmax_pooling2d_4 (MaxPooling2 (None, 63, 63, 64)        0         \n_________________________________________________________________\nconv2d_5 (Conv2D)            (None, 61, 61, 128)       73856     \n_________________________________________________________________\nbatch_normalization_7 (Batch (None, 61, 61, 128)       512       \n_________________________________________________________________\nmax_pooling2d_5 (MaxPooling2 (None, 30, 30, 128)       0         \n_________________________________________________________________\nconv2d_6 (Conv2D)            (None, 28, 28, 256)       295168    \n_________________________________________________________________\nbatch_normalization_8 (Batch (None, 28, 28, 256)       1024      \n_________________________________________________________________\nmax_pooling2d_6 (MaxPooling2 (None, 14, 14, 256)       0         \n_________________________________________________________________\nconv2d_7 (Conv2D)            (None, 12, 12, 256)       590080    \n_________________________________________________________________\nbatch_normalization_9 (Batch (None, 12, 12, 256)       1024      \n_________________________________________________________________\nmax_pooling2d_7 (MaxPooling2 (None, 6, 6, 256)         0         \n_________________________________________________________________\nflatten_1 (Flatten)          (None, 9216)              0         \n_________________________________________________________________\ndense_3 (Dense)              (None, 128)               1179776   \n_________________________________________________________________\nbatch_normalization_10 (Batc (None, 128)               512       \n_________________________________________________________________\ndropout_3 (Dropout)          (None, 128)               0         \n_________________________________________________________________\ndense_4 (Dense)              (None, 64)                8256      \n_________________________________________________________________\nbatch_normalization_11 (Batc (None, 64)                256       \n_________________________________________________________________\ndense_5 (Dense)              (None, 5)                 325       \n=================================================================\nTotal params: 2,152,837\nTrainable params: 2,151,045\nNon-trainable params: 1,792\n_________________________________________________________________\n"
    }
   ],
   "source": [
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Conv2D(64, (3,3), activation=\"relu\", input_shape=(i_s,i_s,3)),\n",
    "    tf.keras.layers.Dropout(0.3),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.MaxPool2D((2,2)),\n",
    "    tf.keras.layers.Conv2D(128, (3,3), activation=\"relu\", kernel_regularizer=regularizers.l2(L2_WEIGHT_DECAY)),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.MaxPool2D(2,2),\n",
    "    tf.keras.layers.Conv2D(256, (3,3), activation=\"relu\"),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.MaxPool2D(2,2),\n",
    "    tf.keras.layers.Conv2D(256, (3,3), activation=\"relu\", kernel_regularizer=regularizers.l2(L2_WEIGHT_DECAY)),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.MaxPool2D(2,2),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(128, activation=\"relu\"),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Dropout(0.25),\n",
    "    tf.keras.layers.Dense(64, activation=\"relu\", kernel_regularizer=regularizers.l2(L2_WEIGHT_DECAY)),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Dense(5, activation=\"softmax\")\n",
    "    ])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def identity_block(input_tensor, kernel_size, filters):\n",
    "    \"\"\"The identity block is the block that has no conv layer at shortcut.\n",
    "    # Arguments\n",
    "        input_tensor: input tensor\n",
    "        kernel_size: default 3, the kernel size of\n",
    "            middle conv layer at main path\n",
    "        filters: list of integers, the filters of 3 conv layer at main path\n",
    "    \"\"\"\n",
    "    filters1, filters2, filters3 = filters\n",
    "\n",
    "    x = layers.Conv2D(filters1, (1, 1), use_bias=False,\n",
    "                      kernel_initializer='he_normal',\n",
    "                      kernel_regularizer=regularizers.l2(L2_WEIGHT_DECAY))(input_tensor)\n",
    "\n",
    "    x = layers.BatchNormalization()(x)\n",
    "\n",
    "    x = layers.Activation('relu')(x)\n",
    "\n",
    "    x = layers.Conv2D(filters2, kernel_size,\n",
    "                      padding='same', use_bias=False,\n",
    "                      kernel_initializer='he_normal',\n",
    "                      kernel_regularizer=regularizers.l2(L2_WEIGHT_DECAY))(x)\n",
    "\n",
    "    x = layers.BatchNormalization()(x)\n",
    "\n",
    "    x = layers.Activation('relu')(x)\n",
    "\n",
    "    x = layers.Conv2D(filters3, (1, 1), use_bias=False,\n",
    "                      kernel_initializer='he_normal',\n",
    "                      kernel_regularizer=regularizers.l2(L2_WEIGHT_DECAY))(x)\n",
    "\n",
    "    x = layers.BatchNormalization()(x)\n",
    "\n",
    "    x = layers.add([x, input_tensor])\n",
    "    x = layers.Activation('relu')(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_block(input_tensor, kernel_size, filters, strides=(2, 2)):\n",
    "    \"\"\"A block that has a conv layer at shortcut.\n",
    "    # Arguments\n",
    "        input_tensor: input tensor\n",
    "        kernel_size: default 3, the kernel size of\n",
    "            middle conv layer at main path\n",
    "        filters: list of integers, the filters of 3 conv layer at main path\n",
    "        stage: integer, current stage label, used for generating layer names\n",
    "    # Returns\n",
    "        Output tensor for the block.\n",
    "    Note that from stage 3,\n",
    "    the second conv layer at main path is with strides=(2, 2)\n",
    "    And the shortcut should have strides=(2, 2) as well\n",
    "    \"\"\"\n",
    "\n",
    "    filters1, filters2, filters3 = filters\n",
    "\n",
    "    x = layers.Conv2D(filters1, (1, 1), use_bias=False,\n",
    "                      kernel_initializer='he_normal',\n",
    "                      kernel_regularizer=regularizers.l2(L2_WEIGHT_DECAY))(input_tensor)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Activation('relu')(x)\n",
    "\n",
    "\n",
    "    x = layers.Conv2D(filters2, kernel_size, strides=strides, padding='same',\n",
    "                      use_bias=False, kernel_initializer='he_normal',\n",
    "                      kernel_regularizer=regularizers.l2(L2_WEIGHT_DECAY))(x)\n",
    "\n",
    "    x = layers.BatchNormalization()(x)\n",
    "\n",
    "    x = layers.Activation('relu')(x)\n",
    "\n",
    "    x = layers.Conv2D(filters3, (1, 1), use_bias=False,\n",
    "                      kernel_initializer='he_normal',\n",
    "                      kernel_regularizer=regularizers.l2(L2_WEIGHT_DECAY))(x)\n",
    "\n",
    "    x = layers.BatchNormalization()(x)\n",
    "\n",
    "    shortcut = layers.Conv2D(filters3, (1, 1), strides=strides, use_bias=False,\n",
    "                             kernel_initializer='he_normal',\n",
    "                      kernel_regularizer=regularizers.l2(L2_WEIGHT_DECAY))(input_tensor)\n",
    "\n",
    "    shortcut = layers.BatchNormalization()(shortcut)\n",
    "\n",
    "    x = layers.add([x, shortcut])\n",
    "    x = layers.Activation('relu')(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resnet50(num_classes, input_shape):\n",
    "    img_input = layers.Input(shape=input_shape)\n",
    "\n",
    "\n",
    "    x = img_input\n",
    "\n",
    "    # Conv1 (7x7,64,stride=2)\n",
    "    x = layers.ZeroPadding2D(padding=(3, 3))(x)\n",
    "\n",
    "    x = layers.Conv2D(64, (7, 7),\n",
    "                      strides=(2, 2),\n",
    "                      padding='valid', use_bias=False,\n",
    "                      kernel_initializer='he_normal',\n",
    "                      kernel_regularizer=regularizers.l2(L2_WEIGHT_DECAY))(x)\n",
    "\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Activation('relu')(x)\n",
    "    x = layers.ZeroPadding2D(padding=(1, 1))(x)\n",
    "\n",
    "    # 3x3 max pool,stride=2\n",
    "    x = layers.MaxPooling2D((3, 3), strides=(2, 2))(x)\n",
    "\n",
    "    # Conv2_x\n",
    "\n",
    "    # 1×1, 64\n",
    "    # 3×3, 64\n",
    "    # 1×1, 256\n",
    "\n",
    "    x = conv_block(x, 3, [64, 64, 256], strides=(1, 1))\n",
    "    x = identity_block(x, 3, [64, 64, 256])\n",
    "    x = identity_block(x, 3, [64, 64, 256])\n",
    "\n",
    "    # Conv3_x\n",
    "    #\n",
    "    # 1×1, 128\n",
    "    # 3×3, 128\n",
    "    # 1×1, 512\n",
    "\n",
    "    x = conv_block(x, 3, [128, 128, 512])\n",
    "    x = identity_block(x, 3, [128, 128, 512])\n",
    "    x = identity_block(x, 3, [128, 128, 512])\n",
    "    x = identity_block(x, 3, [128, 128, 512])\n",
    "\n",
    "    # Conv4_x\n",
    "    # 1×1, 256\n",
    "    # 3×3, 256\n",
    "    # 1×1, 1024\n",
    "    x = conv_block(x, 3, [256, 256, 1024])\n",
    "    x = identity_block(x, 3, [256, 256, 1024])\n",
    "    x = identity_block(x, 3, [256, 256, 1024])\n",
    "    x = identity_block(x, 3, [256, 256, 1024])\n",
    "    x = identity_block(x, 3, [256, 256, 1024])\n",
    "    x = identity_block(x, 3, [256, 256, 1024])\n",
    "\n",
    "    # 1×1, 512\n",
    "    # 3×3, 512\n",
    "    # 1×1, 2048\n",
    "    x = conv_block(x, 3, [512, 512, 2048])\n",
    "    x = identity_block(x, 3, [512, 512, 2048])\n",
    "    x = identity_block(x, 3, [512, 512, 2048])\n",
    "\n",
    "    # average pool, 1000-d fc, softmax\n",
    "    x = layers.GlobalAveragePooling2D()(x)\n",
    "    x = layers.Dense(\n",
    "        num_classes, activation='softmax',\n",
    "            bias_regularizer=regularizers.l2(L2_WEIGHT_DECAY),\n",
    "            kernel_regularizer=regularizers.l2(L2_WEIGHT_DECAY))(x)\n",
    "\n",
    "    # Create model.\n",
    "    return models.Model(img_input, x, name='resnet50')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = resnet50(5, (i_s, i_s, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=\"Adam\", loss='categorical_crossentropy', metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Found 45969 images belonging to 5 classes.\nFound 8087 images belonging to 5 classes.\n"
    }
   ],
   "source": [
    "def convert_to_grayscale(img):\n",
    "      return tf.image.rgb_to_grayscale(img, name=None)\n",
    "# the preprocess function assumes 1 argument, the image, you do not need to add that inline\n",
    "# validation_datagen = ImageDataGenerator(rescale=1./255, preprocessing_function=convert_to_grayscale,)\n",
    "\n",
    "TRAINING_DIR = \"emotions_5/train/\"\n",
    "train_datagen = ImageDataGenerator(\n",
    "      rescale=1./255,\n",
    "      vertical_flip=True)\n",
    "      # rotation_range=40,\n",
    "      # width_shift_range=0.2,\n",
    "      # height_shift_range=0.2,\n",
    "      # shear_range=0.2,\n",
    "      # zoom_range=0.2,\n",
    "      # fill_mode='nearest')\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(TRAINING_DIR, target_size=(i_s,i_s), \n",
    "batch_size=64, class_mode=\"categorical\")\n",
    "\n",
    "VALIDATION_DIR = \"emotions_5/validation/\"\n",
    "validation_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "validation_generator = validation_datagen.flow_from_directory(VALIDATION_DIR, target_size=(i_s,i_s),batch_size=64, class_mode=\"categorical\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bonnie_surprise_1389 image was corrupt, had to be removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_points = \"checkpoint/checkpoint.hb/\"\n",
    "check_point_dir = os.path.dirname(check_points)\n",
    "\n",
    "cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=check_point_dir, verbose=1, monitor=\"val_acc\", save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load checkpoint model\n",
    "# model = tf.keras.models.load_model(\"checkpoint/checkpoint.hb/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Epoch 1/15\n719/719 [==============================] - ETA: 0s - loss: 2.5019 - acc: 0.4190\nEpoch 00001: val_acc improved from -inf to 0.22592, saving model to checkpoint/checkpoint.hb\nINFO:tensorflow:Assets written to: checkpoint/checkpoint.hb\\assets\n719/719 [==============================] - 211s 293ms/step - loss: 2.5019 - acc: 0.4190 - val_loss: 2.1512 - val_acc: 0.2259\nEpoch 2/15\n719/719 [==============================] - ETA: 0s - loss: 1.5060 - acc: 0.5261\nEpoch 00002: val_acc improved from 0.22592 to 0.42105, saving model to checkpoint/checkpoint.hb\nINFO:tensorflow:Assets written to: checkpoint/checkpoint.hb\\assets\n719/719 [==============================] - 210s 292ms/step - loss: 1.5060 - acc: 0.5261 - val_loss: 1.7849 - val_acc: 0.4210\nEpoch 3/15\n719/719 [==============================] - ETA: 0s - loss: 1.3855 - acc: 0.5864\nEpoch 00003: val_acc did not improve from 0.42105\n719/719 [==============================] - 206s 287ms/step - loss: 1.3855 - acc: 0.5864 - val_loss: 1.9649 - val_acc: 0.3370\nEpoch 4/15\n719/719 [==============================] - ETA: 0s - loss: 1.3260 - acc: 0.6229\nEpoch 00004: val_acc did not improve from 0.42105\n719/719 [==============================] - 206s 287ms/step - loss: 1.3260 - acc: 0.6229 - val_loss: 1.9451 - val_acc: 0.2827\nEpoch 5/15\n719/719 [==============================] - ETA: 0s - loss: 1.2399 - acc: 0.6458\nEpoch 00005: val_acc improved from 0.42105 to 0.46804, saving model to checkpoint/checkpoint.hb\nINFO:tensorflow:Assets written to: checkpoint/checkpoint.hb\\assets\n719/719 [==============================] - 213s 296ms/step - loss: 1.2399 - acc: 0.6458 - val_loss: 1.5906 - val_acc: 0.4680\nEpoch 6/15\n719/719 [==============================] - ETA: 0s - loss: 1.2107 - acc: 0.6642\nEpoch 00006: val_acc improved from 0.46804 to 0.48312, saving model to checkpoint/checkpoint.hb\nINFO:tensorflow:Assets written to: checkpoint/checkpoint.hb\\assets\n719/719 [==============================] - 212s 295ms/step - loss: 1.2107 - acc: 0.6642 - val_loss: 1.6223 - val_acc: 0.4831\nEpoch 7/15\n719/719 [==============================] - ETA: 0s - loss: 1.1528 - acc: 0.6808\nEpoch 00007: val_acc improved from 0.48312 to 0.59948, saving model to checkpoint/checkpoint.hb\nINFO:tensorflow:Assets written to: checkpoint/checkpoint.hb\\assets\n719/719 [==============================] - 210s 293ms/step - loss: 1.1528 - acc: 0.6808 - val_loss: 1.3136 - val_acc: 0.5995\nEpoch 8/15\n719/719 [==============================] - ETA: 0s - loss: 1.1579 - acc: 0.6834\nEpoch 00008: val_acc did not improve from 0.59948\n719/719 [==============================] - 205s 285ms/step - loss: 1.1579 - acc: 0.6834 - val_loss: 1.4692 - val_acc: 0.5365\nEpoch 9/15\n719/719 [==============================] - ETA: 0s - loss: 1.0706 - acc: 0.6966\nEpoch 00009: val_acc did not improve from 0.59948\n719/719 [==============================] - 205s 285ms/step - loss: 1.0706 - acc: 0.6966 - val_loss: 1.5160 - val_acc: 0.4994\nEpoch 10/15\n719/719 [==============================] - ETA: 0s - loss: 1.0484 - acc: 0.7080\nEpoch 00010: val_acc improved from 0.59948 to 0.62063, saving model to checkpoint/checkpoint.hb\nINFO:tensorflow:Assets written to: checkpoint/checkpoint.hb\\assets\n719/719 [==============================] - 208s 290ms/step - loss: 1.0484 - acc: 0.7080 - val_loss: 1.2273 - val_acc: 0.6206\nEpoch 11/15\n719/719 [==============================] - ETA: 0s - loss: 1.0231 - acc: 0.7108\nEpoch 00011: val_acc did not improve from 0.62063\n719/719 [==============================] - 203s 283ms/step - loss: 1.0231 - acc: 0.7108 - val_loss: 1.4288 - val_acc: 0.5402\nEpoch 12/15\n719/719 [==============================] - ETA: 0s - loss: 0.9932 - acc: 0.7189\nEpoch 00012: val_acc improved from 0.62063 to 0.64709, saving model to checkpoint/checkpoint.hb\nINFO:tensorflow:Assets written to: checkpoint/checkpoint.hb\\assets\n719/719 [==============================] - 208s 289ms/step - loss: 0.9932 - acc: 0.7189 - val_loss: 1.1745 - val_acc: 0.6471\nEpoch 13/15\n719/719 [==============================] - ETA: 0s - loss: 0.9779 - acc: 0.7227\nEpoch 00013: val_acc did not improve from 0.64709\n719/719 [==============================] - 203s 283ms/step - loss: 0.9779 - acc: 0.7227 - val_loss: 1.4816 - val_acc: 0.4907\nEpoch 14/15\n719/719 [==============================] - ETA: 0s - loss: 1.0422 - acc: 0.7178\nEpoch 00014: val_acc did not improve from 0.64709\n719/719 [==============================] - 203s 283ms/step - loss: 1.0422 - acc: 0.7178 - val_loss: 1.2146 - val_acc: 0.6250\nEpoch 15/15\n719/719 [==============================] - ETA: 0s - loss: 0.9374 - acc: 0.7331\nEpoch 00015: val_acc did not improve from 0.64709\n719/719 [==============================] - 203s 283ms/step - loss: 0.9374 - acc: 0.7331 - val_loss: 1.1648 - val_acc: 0.6409\n"
    }
   ],
   "source": [
    "history = model.fit(train_generator,\n",
    "                            epochs=15,\n",
    "                            verbose=1,  \n",
    "                            validation_data=validation_generator,\n",
    "                            callbacks=[cp_callback]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "INFO:tensorflow:Assets written to: faceNet\\3cnn_2dnn_with_reg\\assets\n"
    }
   ],
   "source": [
    "# Only Run this code if you want to overwrite the old model\n",
    "faceNet_dir = \"faceNet\\\\3cnn_2dnn_with_reg\\\\\"\n",
    "tf.saved_model.save(model, faceNet_dir)\n",
    "# model.save(\"emotion_model.0_3cnn_2dns_input64x64.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load model\n",
    "# model = tf.keras.models.load_model(faceNet_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_dir = \"emotions_5/test/\"\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "test_generator = test_datagen.flow_from_directory(test_dir, target_size=(i_s,i_s), \n",
    "batch_size=32, class_mode=\"categorical\")\n",
    "\n",
    "model.evaluate(test_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PLOT LOSS AND ACCURACY\n",
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.image  as mpimg\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#-----------------------------------------------------------\n",
    "# Retrieve a list of list results on training and test data\n",
    "# sets for each training epoch\n",
    "#-----------------------------------------------------------\n",
    "acc=history.history['acc']\n",
    "val_acc=history.history['val_acc']\n",
    "loss=history.history['loss']\n",
    "val_loss=history.history['val_loss']\n",
    "\n",
    "epochs=range(len(acc)) # Get number of epochs\n",
    "\n",
    "#------------------------------------------------\n",
    "# Plot training and validation accuracy per epoch\n",
    "#------------------------------------------------\n",
    "plt.plot(epochs, acc, 'r', \"Training Accuracy\")\n",
    "plt.plot(epochs, val_acc, 'b', \"Validation Accuracy\")\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.figure()\n",
    "\n",
    "#------------------------------------------------\n",
    "# Plot training and validation loss per epoch\n",
    "#------------------------------------------------\n",
    "plt.plot(epochs, loss, 'r', \"Training Loss\")\n",
    "plt.plot(epochs, val_loss, 'b', \"Validation Loss\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "root_dir = \"img_test\"\n",
    "dir_ = os.listdir(root_dir)\n",
    "\n",
    "test_img_paths = [os.path.join(root_dir,x) for x in dir_]\n",
    "rows = len(dir_)\n",
    "\n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches(16,48)\n",
    "for i, img in enumerate(test_img_paths):\n",
    "    img_load_for_test = image.load_img(img, target_size=(i_s,i_s))\n",
    "    img_test = image.img_to_array(img_load_for_test)\n",
    "    img_test = np.expand_dims(img_test, axis=0)\n",
    "    title = model.predict(img_test)\n",
    "\n",
    "    img_plot = mpimg.imread(img)\n",
    "    sb = plt.subplot(20,2,i+1)\n",
    "    sb.set_title(str(title))\n",
    "    sb.axis(\"off\")\n",
    "    img_plot = mpimg.imread(img)\n",
    "    plt.imshow(img_plot)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import random\n",
    "from   tensorflow.keras.preprocessing.image import img_to_array, load_img\n",
    "\n",
    "# Let's define a new Model that will take an image as input, and will output\n",
    "# intermediate representations for all layers in the previous model after\n",
    "# the first.\n",
    "successive_outputs = [layer.output for layer in model.layers[1:]]\n",
    "\n",
    "#visualization_model = Model(img_input, successive_outputs)\n",
    "visualization_model = tf.keras.models.Model(inputs = model.input, outputs = successive_outputs)\n",
    "\n",
    "# Let's prepare a random input image of a cat or dog from the training set.\n",
    "abc = \"emotions/test/anger\"\n",
    "abc_li = os.listdir(abc)\n",
    "xyz = [os.path.join(abc, f) for f in abc_li]\n",
    "\n",
    "\n",
    "img_path = random.choice(xyz)\n",
    "img = load_img(img_path, target_size=(i_s, i_s))  # this is a PIL image\n",
    "\n",
    "x   = img_to_array(img)                           # Numpy array with shape (150, 150, 3)\n",
    "x   = x.reshape((1,) + x.shape)                   # Numpy array with shape (1, 150, 150, 3)\n",
    "\n",
    "# Rescale by 1/255\n",
    "x /= 255.0\n",
    "\n",
    "# Let's run our image through our network, thus obtaining all\n",
    "# intermediate representations for this image.\n",
    "successive_feature_maps = visualization_model.predict(x)\n",
    "\n",
    "# These are the names of the layers, so can have them as part of our plot\n",
    "layer_names = [layer.name for layer in model.layers]\n",
    "\n",
    "# -----------------------------------------------------------------------\n",
    "# Now let's display our representations\n",
    "# -----------------------------------------------------------------------\n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches(16,48)\n",
    "for layer_name, feature_map in zip(layer_names, successive_feature_maps):\n",
    "  \n",
    "  if len(feature_map.shape) == 4:\n",
    "    \n",
    "    #-------------------------------------------\n",
    "    # Just do this for the conv / maxpool layers, not the fully-connected layers\n",
    "    #-------------------------------------------\n",
    "    n_features = feature_map.shape[-1]  # number of features in the feature map\n",
    "    size       = feature_map.shape[ 1]  # feature map shape (1, size, size, n_features)\n",
    "    \n",
    "    # We will tile our images in this matrix\n",
    "    display_grid = np.zeros((size, size * n_features))\n",
    "    \n",
    "    #-------------------------------------------------\n",
    "    # Postprocess the feature to be visually palatable\n",
    "    #-------------------------------------------------\n",
    "    for i in range(n_features):\n",
    "      x  = feature_map[0, :, :, i]\n",
    "      x -= x.mean()\n",
    "      x /= x.std ()\n",
    "      x *=  64\n",
    "      x += 128\n",
    "      x  = np.clip(x, 0, 255).astype('uint8')\n",
    "      display_grid[:, i * size : (i + 1) * size] = x # Tile each filter into a horizontal grid\n",
    "\n",
    "    #-----------------\n",
    "    # Display the grid\n",
    "    #-----------------\n",
    "\n",
    "    scale = 20. / n_features\n",
    "    plt.figure( figsize=(scale * n_features, scale) )\n",
    "    plt.title ( layer_name )\n",
    "    plt.grid  ( False )\n",
    "    plt.imshow( display_grid, aspect='auto', cmap='viridis') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "tfe",
   "display_name": "tfe"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}