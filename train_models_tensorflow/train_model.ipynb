{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   Copyright 2020 Erik Lee\n",
    "\n",
    "   Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "   you may not use this file except in compliance with the License.\n",
    "   You may obtain a copy of the License at\n",
    "\n",
    "       http://www.apache.org/licenses/LICENSE-2.0\n",
    "\n",
    "   Unless required by applicable law or agreed to in writing, software\n",
    "   distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "   See the License for the specific language governing permissions and\n",
    "   limitations under the License."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import zipfile\n",
    "import random\n",
    "import shutil\n",
    "from shutil import copyfile\n",
    "import cv2.cv2 as cv2\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib as plt\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from stn import spatial_transformer_network as transformer\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import pandas\n",
    "from tensorflow.keras.applications import InceptionV3\n",
    "\n",
    "i_s = 128\n",
    "tf.keras.backend.clear_session()\n",
    "%precision 4\n",
    "\n",
    "L2_WEIGHT_DECAY = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    except RuntimeError as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# model = tf.keras.models.Sequential([\n",
    "#     tf.keras.layers.Conv2D(64, (3,3), activation=\"relu\", input_shape=(i_s,i_s,3)),\n",
    "#     tf.keras.layers.Dropout(0.3),\n",
    "#     tf.keras.layers.BatchNormalization(),\n",
    "#     tf.keras.layers.MaxPool2D((2,2)),\n",
    "#     tf.keras.layers.Conv2D(128, (3,3), activation=\"relu\"),\n",
    "#     tf.keras.layers.BatchNormalization(),\n",
    "#     tf.keras.layers.MaxPool2D(2,2),\n",
    "#     tf.keras.layers.Conv2D(256, (3,3), activation=\"relu\"),\n",
    "#     tf.keras.layers.BatchNormalization(),\n",
    "#     tf.keras.layers.MaxPool2D(2,2),\n",
    "#     tf.keras.layers.Conv2D(256, (3,3), activation=\"relu\"),\n",
    "#     tf.keras.layers.BatchNormalization(),\n",
    "#     tf.keras.layers.MaxPool2D(2,2),\n",
    "#     tf.keras.layers.Flatten(),\n",
    "#     tf.keras.layers.Dense(128, activation=\"relu\"),\n",
    "#     tf.keras.layers.BatchNormalization(),\n",
    "#     tf.keras.layers.Dropout(0.25),\n",
    "#     tf.keras.layers.Dense(64, activation=\"relu\"),\n",
    "#     tf.keras.layers.BatchNormalization(),\n",
    "#     tf.keras.layers.Dense(7, activation=\"softmax\")\n",
    "#     ])\n",
    "# model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# model_inception = InceptionV3(include_top=False, \n",
    "#                     weights=None, \n",
    "#                     input_shape=(256,256,3))\n",
    "\n",
    "# for layer in model_inception.layers:\n",
    "#     layer.trainable = False\n",
    "\n",
    "# model_inception.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# last_layer = model_inception.get_layer(\"mixed7\")\n",
    "\n",
    "# last_output = last_layer.output\n",
    "# print(last_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X = layers.Flatten()(last_output)\n",
    "\n",
    "# X = layers.Dense(1024, activation=\"relu\")(X)\n",
    "\n",
    "# X = layers.Dense(7, activation=\"softmax\")(X)\n",
    "\n",
    "# model = Model(model_inception.input, X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def identity_block(input_tensor, kernel_size, filters):\n",
    "    \"\"\"The identity block is the block that has no conv layer at shortcut.\n",
    "    # Arguments\n",
    "        input_tensor: input tensor\n",
    "        kernel_size: default 3, the kernel size of\n",
    "            middle conv layer at main path\n",
    "        filters: list of integers, the filters of 3 conv layer at main path\n",
    "    \"\"\"\n",
    "    filters1, filters2, filters3 = filters\n",
    "\n",
    "    x = layers.Conv2D(filters1, (1, 1), use_bias=False,\n",
    "                      kernel_initializer='he_normal',\n",
    "                      kernel_regularizer=regularizers.l2(L2_WEIGHT_DECAY))(input_tensor)\n",
    "\n",
    "    x = layers.BatchNormalization()(x)\n",
    "\n",
    "    x = layers.Activation('relu')(x)\n",
    "\n",
    "    x = layers.Conv2D(filters2, kernel_size,\n",
    "                      padding='same', use_bias=False,\n",
    "                      kernel_initializer='he_normal',\n",
    "                      kernel_regularizer=regularizers.l2(L2_WEIGHT_DECAY))(x)\n",
    "\n",
    "    x = layers.BatchNormalization()(x)\n",
    "\n",
    "    x = layers.Activation('relu')(x)\n",
    "\n",
    "    x = layers.Conv2D(filters3, (1, 1), use_bias=False,\n",
    "                      kernel_initializer='he_normal',\n",
    "                      kernel_regularizer=regularizers.l2(L2_WEIGHT_DECAY))(x)\n",
    "\n",
    "    x = layers.BatchNormalization()(x)\n",
    "\n",
    "    x = layers.add([x, input_tensor])\n",
    "    x = layers.Activation('relu')(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_block(input_tensor, kernel_size, filters, strides=(2, 2)):\n",
    "    \"\"\"A block that has a conv layer at shortcut.\n",
    "    # Arguments\n",
    "        input_tensor: input tensor\n",
    "        kernel_size: default 3, the kernel size of\n",
    "            middle conv layer at main path\n",
    "        filters: list of integers, the filters of 3 conv layer at main path\n",
    "        stage: integer, current stage label, used for generating layer names\n",
    "    # Returns\n",
    "        Output tensor for the block.\n",
    "    Note that from stage 3,\n",
    "    the second conv layer at main path is with strides=(2, 2)\n",
    "    And the shortcut should have strides=(2, 2) as well\n",
    "    \"\"\"\n",
    "\n",
    "    filters1, filters2, filters3 = filters\n",
    "\n",
    "    x = layers.Conv2D(filters1, (1, 1), use_bias=False,\n",
    "                      kernel_initializer='he_normal',\n",
    "                      kernel_regularizer=regularizers.l2(L2_WEIGHT_DECAY))(input_tensor)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Activation('relu')(x)\n",
    "\n",
    "\n",
    "    x = layers.Conv2D(filters2, kernel_size, strides=strides, padding='same',\n",
    "                      use_bias=False, kernel_initializer='he_normal',\n",
    "                      kernel_regularizer=regularizers.l2(L2_WEIGHT_DECAY))(x)\n",
    "\n",
    "    x = layers.BatchNormalization()(x)\n",
    "\n",
    "    x = layers.Activation('relu')(x)\n",
    "\n",
    "    x = layers.Conv2D(filters3, (1, 1), use_bias=False,\n",
    "                      kernel_initializer='he_normal',\n",
    "                      kernel_regularizer=regularizers.l2(L2_WEIGHT_DECAY))(x)\n",
    "\n",
    "    x = layers.BatchNormalization()(x)\n",
    "\n",
    "    shortcut = layers.Conv2D(filters3, (1, 1), strides=strides, use_bias=False,\n",
    "                             kernel_initializer='he_normal',\n",
    "                      kernel_regularizer=regularizers.l2(L2_WEIGHT_DECAY))(input_tensor)\n",
    "\n",
    "    shortcut = layers.BatchNormalization()(shortcut)\n",
    "\n",
    "    x = layers.add([x, shortcut])\n",
    "    x = layers.Activation('relu')(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resnet50(num_classes, input_shape):\n",
    "    img_input = layers.Input(shape=input_shape)\n",
    "\n",
    "\n",
    "    x = img_input\n",
    "\n",
    "    # Conv1 (7x7,64,stride=2)\n",
    "    x = layers.ZeroPadding2D(padding=(3, 3))(x)\n",
    "\n",
    "    x = layers.Conv2D(64, (7, 7),\n",
    "                      strides=(2, 2),\n",
    "                      padding='valid', use_bias=False,\n",
    "                      kernel_initializer='he_normal',\n",
    "                      kernel_regularizer=regularizers.l2(L2_WEIGHT_DECAY))(x)\n",
    "\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Activation('relu')(x)\n",
    "    x = layers.ZeroPadding2D(padding=(1, 1))(x)\n",
    "\n",
    "    # 3x3 max pool,stride=2\n",
    "    x = layers.MaxPooling2D((3, 3), strides=(2, 2))(x)\n",
    "\n",
    "    # Conv2_x\n",
    "\n",
    "    # 1×1, 64\n",
    "    # 3×3, 64\n",
    "    # 1×1, 256\n",
    "\n",
    "    x = conv_block(x, 3, [64, 64, 256], strides=(1, 1))\n",
    "    x = identity_block(x, 3, [64, 64, 256])\n",
    "    x = identity_block(x, 3, [64, 64, 256])\n",
    "\n",
    "    # Conv3_x\n",
    "    #\n",
    "    # 1×1, 128\n",
    "    # 3×3, 128\n",
    "    # 1×1, 512\n",
    "\n",
    "    x = conv_block(x, 3, [128, 128, 512])\n",
    "    x = identity_block(x, 3, [128, 128, 512])\n",
    "    x = identity_block(x, 3, [128, 128, 512])\n",
    "    x = identity_block(x, 3, [128, 128, 512])\n",
    "\n",
    "    # Conv4_x\n",
    "    # 1×1, 256\n",
    "    # 3×3, 256\n",
    "    # 1×1, 1024\n",
    "    x = conv_block(x, 3, [256, 256, 1024])\n",
    "    x = identity_block(x, 3, [256, 256, 1024])\n",
    "    x = identity_block(x, 3, [256, 256, 1024])\n",
    "    x = identity_block(x, 3, [256, 256, 1024])\n",
    "    x = identity_block(x, 3, [256, 256, 1024])\n",
    "    x = identity_block(x, 3, [256, 256, 1024])\n",
    "\n",
    "    # 1×1, 512\n",
    "    # 3×3, 512\n",
    "    # 1×1, 2048\n",
    "    x = conv_block(x, 3, [512, 512, 2048])\n",
    "    x = identity_block(x, 3, [512, 512, 2048])\n",
    "    x = identity_block(x, 3, [512, 512, 2048])\n",
    "\n",
    "    # average pool, 1000-d fc, softmax\n",
    "    x = layers.GlobalAveragePooling2D()(x)\n",
    "    x = layers.Dense(\n",
    "        num_classes, activation='softmax',\n",
    "            bias_regularizer=regularizers.l2(L2_WEIGHT_DECAY),\n",
    "            kernel_regularizer=regularizers.l2(L2_WEIGHT_DECAY))(x)\n",
    "\n",
    "    # Create model.\n",
    "    return models.Model(img_input, x, name='resnet50')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = resnet50(7, (i_s, i_s, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=\"Adam\", loss='categorical_crossentropy', metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Found 46056 images belonging to 7 classes.\nFound 8088 images belonging to 7 classes.\n"
    }
   ],
   "source": [
    "def convert_to_grayscale(img):\n",
    "      return tf.image.rgb_to_grayscale(img, name=None)\n",
    "# the preprocess function assumes 1 argument, the image, you do not need to add that inline\n",
    "# validation_datagen = ImageDataGenerator(rescale=1./255, preprocessing_function=convert_to_grayscale,)\n",
    "\n",
    "TRAINING_DIR = \"emotions/train/\"\n",
    "train_datagen = ImageDataGenerator(\n",
    "      rescale=1./255,\n",
    "      vertical_flip=True)\n",
    "      # rotation_range=40,\n",
    "      # width_shift_range=0.2,\n",
    "      # height_shift_range=0.2,\n",
    "      # shear_range=0.2,\n",
    "      # zoom_range=0.2,\n",
    "      # fill_mode='nearest')\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(TRAINING_DIR, target_size=(i_s,i_s), \n",
    "batch_size=64, class_mode=\"categorical\")\n",
    "\n",
    "VALIDATION_DIR = \"emotions/validation/\"\n",
    "validation_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "validation_generator = validation_datagen.flow_from_directory(VALIDATION_DIR, target_size=(i_s,i_s),batch_size=64, class_mode=\"categorical\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bonnie_surprise_1389 image was corrupt, had to be removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_points = \"checkpoint/checkpoint.hb/\"\n",
    "check_point_dir = os.path.dirname(check_points)\n",
    "\n",
    "cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=check_point_dir, verbose=1, monitor=\"val_acc\", save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load checkpoint model\n",
    "# model = tf.keras.models.load_model(\"checkpoint/checkpoint.hb/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Epoch 1/10\n720/720 [==============================] - ETA: 0s - loss: 1.5276 - acc: 0.4485\nEpoch 00001: val_acc improved from -inf to 0.40146, saving model to checkpoint/checkpoint.hb\nWARNING:tensorflow:From c:\\coding\\real_time_video_classifier\\tfe\\lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py:1813: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\nInstructions for updating:\nIf using Keras pass *_constraint arguments to layers.\nINFO:tensorflow:Assets written to: checkpoint/checkpoint.hb\\assets\n720/720 [==============================] - 245s 340ms/step - loss: 1.5276 - acc: 0.4485 - val_loss: 1.7332 - val_acc: 0.4015\nEpoch 2/10\n720/720 [==============================] - ETA: 0s - loss: 1.0061 - acc: 0.6245\nEpoch 00002: val_acc improved from 0.40146 to 0.55082, saving model to checkpoint/checkpoint.hb\nINFO:tensorflow:Assets written to: checkpoint/checkpoint.hb\\assets\n720/720 [==============================] - 225s 312ms/step - loss: 1.0061 - acc: 0.6245 - val_loss: 1.2196 - val_acc: 0.5508\nEpoch 3/10\n720/720 [==============================] - ETA: 0s - loss: 0.8818 - acc: 0.6719\nEpoch 00003: val_acc improved from 0.55082 to 0.56170, saving model to checkpoint/checkpoint.hb\nINFO:tensorflow:Assets written to: checkpoint/checkpoint.hb\\assets\n720/720 [==============================] - 230s 319ms/step - loss: 0.8818 - acc: 0.6719 - val_loss: 1.2319 - val_acc: 0.5617\nEpoch 4/10\n720/720 [==============================] - ETA: 0s - loss: 0.7898 - acc: 0.7048\nEpoch 00004: val_acc improved from 0.56170 to 0.60398, saving model to checkpoint/checkpoint.hb\nINFO:tensorflow:Assets written to: checkpoint/checkpoint.hb\\assets\n720/720 [==============================] - 230s 320ms/step - loss: 0.7898 - acc: 0.7048 - val_loss: 1.4295 - val_acc: 0.6040\nEpoch 5/10\n720/720 [==============================] - ETA: 0s - loss: 0.7702 - acc: 0.7137\nEpoch 00005: val_acc did not improve from 0.60398\n720/720 [==============================] - 197s 273ms/step - loss: 0.7702 - acc: 0.7137 - val_loss: 1.2612 - val_acc: 0.5985\nEpoch 6/10\n720/720 [==============================] - ETA: 0s - loss: 0.7234 - acc: 0.7322\nEpoch 00006: val_acc improved from 0.60398 to 0.64800, saving model to checkpoint/checkpoint.hb\nINFO:tensorflow:Assets written to: checkpoint/checkpoint.hb\\assets\n720/720 [==============================] - 230s 320ms/step - loss: 0.7234 - acc: 0.7322 - val_loss: 0.9227 - val_acc: 0.6480\nEpoch 7/10\n720/720 [==============================] - ETA: 0s - loss: 0.6674 - acc: 0.7516\nEpoch 00007: val_acc did not improve from 0.64800\n720/720 [==============================] - 199s 276ms/step - loss: 0.6674 - acc: 0.7516 - val_loss: 1.0099 - val_acc: 0.6128\nEpoch 8/10\n720/720 [==============================] - ETA: 0s - loss: 0.6365 - acc: 0.7635\nEpoch 00008: val_acc did not improve from 0.64800\n720/720 [==============================] - 198s 275ms/step - loss: 0.6365 - acc: 0.7635 - val_loss: 1.1425 - val_acc: 0.6116\nEpoch 9/10\n720/720 [==============================] - ETA: 0s - loss: 0.6189 - acc: 0.7710\nEpoch 00009: val_acc improved from 0.64800 to 0.66395, saving model to checkpoint/checkpoint.hb\nINFO:tensorflow:Assets written to: checkpoint/checkpoint.hb\\assets\n720/720 [==============================] - 232s 323ms/step - loss: 0.6189 - acc: 0.7710 - val_loss: 0.8977 - val_acc: 0.6639\nEpoch 10/10\n720/720 [==============================] - ETA: 0s - loss: 0.5643 - acc: 0.7907\nEpoch 00010: val_acc did not improve from 0.66395\n720/720 [==============================] - 198s 275ms/step - loss: 0.5643 - acc: 0.7907 - val_loss: 0.9227 - val_acc: 0.6562\n"
    }
   ],
   "source": [
    "history = model.fit(train_generator,\n",
    "                            epochs=10,\n",
    "                            verbose=1,  \n",
    "                            validation_data=validation_generator,\n",
    "                            callbacks=[cp_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "INFO:tensorflow:Assets written to: faceNet\\resNet50\\assets\n"
    }
   ],
   "source": [
    "# Only Run this code if you want to overwrite the old model\n",
    "faceNet_dir = \"faceNet\\\\resNet50\\\\\"\n",
    "tf.saved_model.save(model, faceNet_dir)\n",
    "# model.save(\"emotion_model.0_3cnn_2dns_input64x64.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load model\n",
    "# model = tf.keras.models.load_model(faceNet_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_dir = \"emotions/test/\"\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "test_generator = test_datagen.flow_from_directory(test_dir, target_size=(i_s,i_s), \n",
    "batch_size=32, class_mode=\"categorical\")\n",
    "\n",
    "model.evaluate(test_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PLOT LOSS AND ACCURACY\n",
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.image  as mpimg\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#-----------------------------------------------------------\n",
    "# Retrieve a list of list results on training and test data\n",
    "# sets for each training epoch\n",
    "#-----------------------------------------------------------\n",
    "acc=history.history['acc']\n",
    "val_acc=history.history['val_acc']\n",
    "loss=history.history['loss']\n",
    "val_loss=history.history['val_loss']\n",
    "\n",
    "epochs=range(len(acc)) # Get number of epochs\n",
    "\n",
    "#------------------------------------------------\n",
    "# Plot training and validation accuracy per epoch\n",
    "#------------------------------------------------\n",
    "plt.plot(epochs, acc, 'r', \"Training Accuracy\")\n",
    "plt.plot(epochs, val_acc, 'b', \"Validation Accuracy\")\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.figure()\n",
    "\n",
    "#------------------------------------------------\n",
    "# Plot training and validation loss per epoch\n",
    "#------------------------------------------------\n",
    "plt.plot(epochs, loss, 'r', \"Training Loss\")\n",
    "plt.plot(epochs, val_loss, 'b', \"Validation Loss\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "root_dir = \"img_test\"\n",
    "dir_ = os.listdir(root_dir)\n",
    "\n",
    "test_img_paths = [os.path.join(root_dir,x) for x in dir_]\n",
    "rows = len(dir_)\n",
    "\n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches(16,48)\n",
    "for i, img in enumerate(test_img_paths):\n",
    "    img_load_for_test = image.load_img(img, target_size=(i_s,i_s))\n",
    "    img_test = image.img_to_array(img_load_for_test)\n",
    "    img_test = np.expand_dims(img_test, axis=0)\n",
    "    title = model.predict(img_test)\n",
    "\n",
    "    img_plot = mpimg.imread(img)\n",
    "    sb = plt.subplot(20,2,i+1)\n",
    "    sb.set_title(str(title))\n",
    "    sb.axis(\"off\")\n",
    "    img_plot = mpimg.imread(img)\n",
    "    plt.imshow(img_plot)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import random\n",
    "from   tensorflow.keras.preprocessing.image import img_to_array, load_img\n",
    "\n",
    "# Let's define a new Model that will take an image as input, and will output\n",
    "# intermediate representations for all layers in the previous model after\n",
    "# the first.\n",
    "successive_outputs = [layer.output for layer in model.layers[1:]]\n",
    "\n",
    "#visualization_model = Model(img_input, successive_outputs)\n",
    "visualization_model = tf.keras.models.Model(inputs = model.input, outputs = successive_outputs)\n",
    "\n",
    "# Let's prepare a random input image of a cat or dog from the training set.\n",
    "abc = \"emotions/test/anger\"\n",
    "abc_li = os.listdir(abc)\n",
    "xyz = [os.path.join(abc, f) for f in abc_li]\n",
    "\n",
    "\n",
    "img_path = random.choice(xyz)\n",
    "img = load_img(img_path, target_size=(i_s, i_s))  # this is a PIL image\n",
    "\n",
    "x   = img_to_array(img)                           # Numpy array with shape (150, 150, 3)\n",
    "x   = x.reshape((1,) + x.shape)                   # Numpy array with shape (1, 150, 150, 3)\n",
    "\n",
    "# Rescale by 1/255\n",
    "x /= 255.0\n",
    "\n",
    "# Let's run our image through our network, thus obtaining all\n",
    "# intermediate representations for this image.\n",
    "successive_feature_maps = visualization_model.predict(x)\n",
    "\n",
    "# These are the names of the layers, so can have them as part of our plot\n",
    "layer_names = [layer.name for layer in model.layers]\n",
    "\n",
    "# -----------------------------------------------------------------------\n",
    "# Now let's display our representations\n",
    "# -----------------------------------------------------------------------\n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches(16,48)\n",
    "for layer_name, feature_map in zip(layer_names, successive_feature_maps):\n",
    "  \n",
    "  if len(feature_map.shape) == 4:\n",
    "    \n",
    "    #-------------------------------------------\n",
    "    # Just do this for the conv / maxpool layers, not the fully-connected layers\n",
    "    #-------------------------------------------\n",
    "    n_features = feature_map.shape[-1]  # number of features in the feature map\n",
    "    size       = feature_map.shape[ 1]  # feature map shape (1, size, size, n_features)\n",
    "    \n",
    "    # We will tile our images in this matrix\n",
    "    display_grid = np.zeros((size, size * n_features))\n",
    "    \n",
    "    #-------------------------------------------------\n",
    "    # Postprocess the feature to be visually palatable\n",
    "    #-------------------------------------------------\n",
    "    for i in range(n_features):\n",
    "      x  = feature_map[0, :, :, i]\n",
    "      x -= x.mean()\n",
    "      x /= x.std ()\n",
    "      x *=  64\n",
    "      x += 128\n",
    "      x  = np.clip(x, 0, 255).astype('uint8')\n",
    "      display_grid[:, i * size : (i + 1) * size] = x # Tile each filter into a horizontal grid\n",
    "\n",
    "    #-----------------\n",
    "    # Display the grid\n",
    "    #-----------------\n",
    "\n",
    "    scale = 20. / n_features\n",
    "    plt.figure( figsize=(scale * n_features, scale) )\n",
    "    plt.title ( layer_name )\n",
    "    plt.grid  ( False )\n",
    "    plt.imshow( display_grid, aspect='auto', cmap='viridis') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "tfe",
   "display_name": "tfe"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}