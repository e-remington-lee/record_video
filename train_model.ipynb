{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   Copyright 2020 Erik Lee\n",
    "\n",
    "   Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "   you may not use this file except in compliance with the License.\n",
    "   You may obtain a copy of the License at\n",
    "\n",
    "       http://www.apache.org/licenses/LICENSE-2.0\n",
    "\n",
    "   Unless required by applicable law or agreed to in writing, software\n",
    "   distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "   See the License for the specific language governing permissions and\n",
    "   limitations under the License."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "'%.4f'"
     },
     "metadata": {},
     "execution_count": 1
    }
   ],
   "source": [
    "import os\n",
    "import zipfile\n",
    "import random\n",
    "import shutil\n",
    "from shutil import copyfile\n",
    "import cv2.cv2 as cv2\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib as plt\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from stn import spatial_transformer_network as transformer\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import pandas\n",
    "from tensorflow.keras.applications import InceptionV3\n",
    "\n",
    "i_s = 128\n",
    "tf.keras.backend.clear_session()\n",
    "%precision 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "cpus = tf.config.experimental.list_physical_devices('CPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    except RuntimeError as e:\n",
    "        print(e)\n",
    "else:\n",
    "    try:\n",
    "        for cpu in cpus:\n",
    "            tf.config.experimental.set_memory_growth(cpu, True)\n",
    "    except RuntimeError as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Model: \"sequential_1\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nconv2d_4 (Conv2D)            (None, 126, 126, 64)      1792      \n_________________________________________________________________\nbatch_normalization_6 (Batch (None, 126, 126, 64)      256       \n_________________________________________________________________\ndropout_2 (Dropout)          (None, 126, 126, 64)      0         \n_________________________________________________________________\nmax_pooling2d_4 (MaxPooling2 (None, 63, 63, 64)        0         \n_________________________________________________________________\nconv2d_5 (Conv2D)            (None, 61, 61, 128)       73856     \n_________________________________________________________________\nbatch_normalization_7 (Batch (None, 61, 61, 128)       512       \n_________________________________________________________________\nmax_pooling2d_5 (MaxPooling2 (None, 30, 30, 128)       0         \n_________________________________________________________________\nconv2d_6 (Conv2D)            (None, 28, 28, 256)       295168    \n_________________________________________________________________\nbatch_normalization_8 (Batch (None, 28, 28, 256)       1024      \n_________________________________________________________________\nmax_pooling2d_6 (MaxPooling2 (None, 14, 14, 256)       0         \n_________________________________________________________________\nconv2d_7 (Conv2D)            (None, 12, 12, 256)       590080    \n_________________________________________________________________\nbatch_normalization_9 (Batch (None, 12, 12, 256)       1024      \n_________________________________________________________________\nmax_pooling2d_7 (MaxPooling2 (None, 6, 6, 256)         0         \n_________________________________________________________________\nflatten_1 (Flatten)          (None, 9216)              0         \n_________________________________________________________________\ndense_3 (Dense)              (None, 128)               1179776   \n_________________________________________________________________\nbatch_normalization_10 (Batc (None, 128)               512       \n_________________________________________________________________\ndropout_3 (Dropout)          (None, 128)               0         \n_________________________________________________________________\ndense_4 (Dense)              (None, 64)                8256      \n_________________________________________________________________\nbatch_normalization_11 (Batc (None, 64)                256       \n_________________________________________________________________\ndense_5 (Dense)              (None, 7)                 455       \n=================================================================\nTotal params: 2,152,967\nTrainable params: 2,151,175\nNon-trainable params: 1,792\n_________________________________________________________________\n"
    }
   ],
   "source": [
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Conv2D(64, (3,3), activation=\"relu\", input_shape=(i_s,i_s,3)),\n",
    "    tf.keras.layers.BatchNormalization(axis=-1),\n",
    "    tf.keras.layers.Dropout(0.3),\n",
    "    tf.keras.layers.MaxPool2D((2,2)),\n",
    "    tf.keras.layers.Conv2D(128, (3,3), activation=\"relu\"),\n",
    "    tf.keras.layers.BatchNormalization(axis=-1),\n",
    "    tf.keras.layers.MaxPool2D(2,2),\n",
    "    tf.keras.layers.Conv2D(256, (3,3), activation=\"relu\"),\n",
    "    tf.keras.layers.BatchNormalization(axis=-1),\n",
    "    tf.keras.layers.MaxPool2D(2,2),\n",
    "    tf.keras.layers.Conv2D(256, (3,3), activation=\"relu\"),\n",
    "    tf.keras.layers.BatchNormalization(axis=-1),\n",
    "    tf.keras.layers.MaxPool2D(2,2),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(128, activation=\"relu\"),\n",
    "    tf.keras.layers.BatchNormalization(axis=-1),\n",
    "    tf.keras.layers.Dropout(0.25),\n",
    "    tf.keras.layers.Dense(64, activation=\"relu\"),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Dense(7, activation=\"softmax\")\n",
    "    ])\n",
    "model.summary()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# model.add(tf.keras.layers.Dense(1024, activation=\"relu\"))\n",
    "\n",
    "# model.add(tf.keras.layers.Dense(512, activation=\"relu\"))\n",
    "\n",
    "# model.add(tf.keras.layers.Dense(7, activation=\"softmax\"))\n",
    "\n",
    "# model = tf.keras.models.Sequential([\n",
    "#     tf.keras.layers.Conv2D(20, (2,2), activation=\"relu\", input_shape=(64,64,3), strides=2),\n",
    "#     tf.keras.layers.MaxPool2D(2,2),\n",
    "#     tf.keras.layers.Conv2D(50, (2,2), activation=\"relu\", strides=2),\n",
    "#     tf.keras.layers.MaxPool2D(2,2),\n",
    "#     tf.keras.layers.Flatten(),\n",
    "#     tf.keras.layers.Dense(500, activation=\"relu\"),\n",
    "#     tf.keras.layers.Dense(7, activation=\"softmax\"),\n",
    "# ])\n",
    "# model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# model_inception = InceptionV3(include_top=False, \n",
    "#                     weights=None, \n",
    "#                     input_shape=(256,256,3))\n",
    "\n",
    "# for layer in model_inception.layers:\n",
    "#     layer.trainable = False\n",
    "\n",
    "# model_inception.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# last_layer = model_inception.get_layer(\"mixed7\")\n",
    "\n",
    "# last_output = last_layer.output\n",
    "# print(last_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X = layers.Flatten()(last_output)\n",
    "\n",
    "# X = layers.Dense(1024, activation=\"relu\")(X)\n",
    "\n",
    "# X = layers.Dense(7, activation=\"softmax\")(X)\n",
    "\n",
    "# model = Model(model_inception.input, X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=\"Adam\", loss='categorical_crossentropy', metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Found 25226 images belonging to 7 classes.\nFound 5436 images belonging to 7 classes.\n"
    }
   ],
   "source": [
    "def convert_to_grayscale(img):\n",
    "      return tf.image.rgb_to_grayscale(img, name=None)\n",
    "# the preprocess function assumes 1 argument, the image, you do not need to add that inline\n",
    "# validation_datagen = ImageDataGenerator(rescale=1./255, preprocessing_function=convert_to_grayscale,)\n",
    "\n",
    "TRAINING_DIR = \"emotions/train/\"\n",
    "train_datagen = ImageDataGenerator(\n",
    "      rescale=1./255,\n",
    "      horizontal_flip=True)\n",
    "      # rotation_range=40,\n",
    "      # width_shift_range=0.2,\n",
    "      # height_shift_range=0.2,\n",
    "      # shear_range=0.2,\n",
    "      # zoom_range=0.2,\n",
    "      # fill_mode='nearest')\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(TRAINING_DIR, target_size=(i_s,i_s), \n",
    "batch_size=32, class_mode=\"categorical\")\n",
    "\n",
    "VALIDATION_DIR = \"emotions/validation/\"\n",
    "validation_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "validation_generator = validation_datagen.flow_from_directory(VALIDATION_DIR, target_size=(i_s,i_s),batch_size=64, class_mode=\"categorical\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bonnie_surprise_1389 image was corrupt, had to be removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_points = \"checkpoint/checkpoint.hb/\"\n",
    "check_point_dir = os.path.dirname(check_points)\n",
    "\n",
    "cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=check_point_dir, verbose=1, monitor=\"val_acc\", save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load checkpoint model\n",
    "model = tf.keras.models.load_model(\"checkpoint/checkpoint.hb/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "history = model.fit(train_generator,\n",
    "                            epochs=15,\n",
    "                            verbose=1,  \n",
    "                            validation_data=validation_generator,\n",
    "                            callbacks=[cp_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "INFO:tensorflow:Assets written to: faceNet\\1\\assets\n"
    }
   ],
   "source": [
    "# Only Run this code if you want to overwrite the old model\n",
    "# faceNet_dir = \"faceNet\\\\1\\\\\"\n",
    "# tf.saved_model.save(model, faceNet_dir)\n",
    "# model.save(\"emotion_model.0_3cnn_2dns_input64x64.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load model\n",
    "model = tf.keras.models.load_model(faceNet_dir)\n",
    "# model =tf.keras.models.load_model(\"emotion_model_v1.0_3cnn_2dns_input64x64.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Found 5438 images belonging to 7 classes.\n170/170 [==============================] - 3s 16ms/step - loss: 1.2217 - acc: 0.5235\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "[1.2217, 0.5235]"
     },
     "metadata": {},
     "execution_count": 33
    }
   ],
   "source": [
    "test_dir = \"emotions/test/\"\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "test_generator = test_datagen.flow_from_directory(test_dir, target_size=(i_s,i_s), \n",
    "batch_size=32, class_mode=\"categorical\")\n",
    "\n",
    "model.evaluate(test_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PLOT LOSS AND ACCURACY\n",
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.image  as mpimg\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#-----------------------------------------------------------\n",
    "# Retrieve a list of list results on training and test data\n",
    "# sets for each training epoch\n",
    "#-----------------------------------------------------------\n",
    "acc=history.history['acc']\n",
    "val_acc=history.history['val_acc']\n",
    "loss=history.history['loss']\n",
    "val_loss=history.history['val_loss']\n",
    "\n",
    "epochs=range(len(acc)) # Get number of epochs\n",
    "\n",
    "#------------------------------------------------\n",
    "# Plot training and validation accuracy per epoch\n",
    "#------------------------------------------------\n",
    "plt.plot(epochs, acc, 'r', \"Training Accuracy\")\n",
    "plt.plot(epochs, val_acc, 'b', \"Validation Accuracy\")\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.figure()\n",
    "\n",
    "#------------------------------------------------\n",
    "# Plot training and validation loss per epoch\n",
    "#------------------------------------------------\n",
    "plt.plot(epochs, loss, 'r', \"Training Loss\")\n",
    "plt.plot(epochs, val_loss, 'b', \"Validation Loss\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "root_dir = \"img_test\"\n",
    "dir_ = os.listdir(root_dir)\n",
    "\n",
    "test_img_paths = [os.path.join(root_dir,x) for x in dir_]\n",
    "rows = len(dir_)\n",
    "\n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches(16,48)\n",
    "for i, img in enumerate(test_img_paths):\n",
    "    img_load_for_test = image.load_img(img, target_size=(i_s,i_s))\n",
    "    img_test = image.img_to_array(img_load_for_test)\n",
    "    img_test = np.expand_dims(img_test, axis=0)\n",
    "    title = model.predict(img_test)\n",
    "\n",
    "    img_plot = mpimg.imread(img)\n",
    "    sb = plt.subplot(20,2,i+1)\n",
    "    sb.set_title(str(title))\n",
    "    sb.axis(\"off\")\n",
    "    img_plot = mpimg.imread(img)\n",
    "    plt.imshow(img_plot)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import random\n",
    "from   tensorflow.keras.preprocessing.image import img_to_array, load_img\n",
    "\n",
    "# Let's define a new Model that will take an image as input, and will output\n",
    "# intermediate representations for all layers in the previous model after\n",
    "# the first.\n",
    "successive_outputs = [layer.output for layer in model.layers[1:]]\n",
    "\n",
    "#visualization_model = Model(img_input, successive_outputs)\n",
    "visualization_model = tf.keras.models.Model(inputs = model.input, outputs = successive_outputs)\n",
    "\n",
    "# Let's prepare a random input image of a cat or dog from the training set.\n",
    "abc = \"emotions/test/anger\"\n",
    "abc_li = os.listdir(abc)\n",
    "xyz = [os.path.join(abc, f) for f in abc_li]\n",
    "\n",
    "\n",
    "img_path = random.choice(xyz)\n",
    "img = load_img(img_path, target_size=(i_s, i_s))  # this is a PIL image\n",
    "\n",
    "x   = img_to_array(img)                           # Numpy array with shape (150, 150, 3)\n",
    "x   = x.reshape((1,) + x.shape)                   # Numpy array with shape (1, 150, 150, 3)\n",
    "\n",
    "# Rescale by 1/255\n",
    "x /= 255.0\n",
    "\n",
    "# Let's run our image through our network, thus obtaining all\n",
    "# intermediate representations for this image.\n",
    "successive_feature_maps = visualization_model.predict(x)\n",
    "\n",
    "# These are the names of the layers, so can have them as part of our plot\n",
    "layer_names = [layer.name for layer in model.layers]\n",
    "\n",
    "# -----------------------------------------------------------------------\n",
    "# Now let's display our representations\n",
    "# -----------------------------------------------------------------------\n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches(16,48)\n",
    "for layer_name, feature_map in zip(layer_names, successive_feature_maps):\n",
    "  \n",
    "  if len(feature_map.shape) == 4:\n",
    "    \n",
    "    #-------------------------------------------\n",
    "    # Just do this for the conv / maxpool layers, not the fully-connected layers\n",
    "    #-------------------------------------------\n",
    "    n_features = feature_map.shape[-1]  # number of features in the feature map\n",
    "    size       = feature_map.shape[ 1]  # feature map shape (1, size, size, n_features)\n",
    "    \n",
    "    # We will tile our images in this matrix\n",
    "    display_grid = np.zeros((size, size * n_features))\n",
    "    \n",
    "    #-------------------------------------------------\n",
    "    # Postprocess the feature to be visually palatable\n",
    "    #-------------------------------------------------\n",
    "    for i in range(n_features):\n",
    "      x  = feature_map[0, :, :, i]\n",
    "      x -= x.mean()\n",
    "      x /= x.std ()\n",
    "      x *=  64\n",
    "      x += 128\n",
    "      x  = np.clip(x, 0, 255).astype('uint8')\n",
    "      display_grid[:, i * size : (i + 1) * size] = x # Tile each filter into a horizontal grid\n",
    "\n",
    "    #-----------------\n",
    "    # Display the grid\n",
    "    #-----------------\n",
    "\n",
    "    scale = 20. / n_features\n",
    "    plt.figure( figsize=(scale * n_features, scale) )\n",
    "    plt.title ( layer_name )\n",
    "    plt.grid  ( False )\n",
    "    plt.imshow( display_grid, aspect='auto', cmap='viridis') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "tfe",
   "display_name": "tfe"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}